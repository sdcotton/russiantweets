{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up environment...\n",
      "set up environment\n",
      "loading tweets from file...\n",
      "loaded tweets from file\n",
      "selecting sample of tweets...\n",
      "selected sample of tweets\n",
      "tokenizing tweets...\n",
      "tokenized tweets\n",
      "stemming tweets...\n",
      "stemmed tweets\n",
      "removing stopwords...\n",
      "removed stopwords\n",
      "removing puncutation...\n",
      "removed punctuation\n",
      "removing @someones...\n",
      "removed @someones\n",
      "removing links...\n",
      "removed links\n",
      "removing hashtags...\n",
      "removed hashtags\n",
      "removing retweets\n",
      "removed retweets\n",
      "generating word count...\n",
      "generated word count\n",
      "{'us': 0.40705563093622793, 'stand': 0.27137042062415195, 'trump': 2.306648575305292, 'peopl': 0.40705563093622793, 'hillari': 0.6784260515603799, 'go': 0.47489823609226595, 'vote': 0.40705563093622793, 'clinton': 1.5603799185888738, 'amp': 0.746268656716418, 'never': 0.33921302578018997, 'would': 0.33921302578018997, 'one': 0.5427408412483039, 'make': 0.40705563093622793, 'u': 0.33921302578018997, 'say': 0.40705563093622793, 'new': 0.40705563093622793, 'obama': 0.13568521031207598, 'via': 0.33921302578018997, 'get': 0.6105834464043419, 'like': 0.40705563093622793, 'black': 0.33921302578018997, 'democrat': 0.27137042062415195, 'need': 0.33921302578018997, 'tell': 0.33921302578018997, 'live': 0.33921302578018997, 'donald': 0.47489823609226595, 'ralli': 0.33921302578018997, 'elect': 0.20352781546811397, 'love': 0.33921302578018997, 'day': 0.40705563093622793, 'republican': 0.33921302578018997, 'poll': 0.40705563093622793}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "def settingenv():\n",
    "    ''' download nltk packages '''\n",
    "    if not nltk.data.find('corpora/stopwords'):\n",
    "        nltk.download('stopwords')\n",
    "    # I wasn't able to download the word_tokenize, don't know pat\n",
    "#    nltk.download('word_tokenize')\n",
    "    if not nltk.data.find('tokenizers/punkt'):\n",
    "        nltk.download('punkt')\n",
    "\n",
    "def tweets(path = \"tweets.csv\"):\n",
    "    '''import tweets csv into a pandas dataframe'''\n",
    "    tweets = pd.read_csv(path)\n",
    "    return tweets\n",
    "\n",
    "def sampling(tweets_df, count=20):\n",
    "    '''\n",
    "    slice a sample from the original tweets\n",
    "    tweets: tweets data frame\n",
    "    a, b: range of sampling\n",
    "    returns a corpus data frame.\n",
    "    '''\n",
    "    if count <= 0:\n",
    "        sample = tweets_df\n",
    "    else:\n",
    "        sample = tweets_df.sample(n=count)\n",
    "    corpus = sample.loc[:,['text']]\n",
    "    corpus['text_index'] = corpus.index\n",
    "    corpus.text = corpus.text.astype(str)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "def tokenize(insent):\n",
    "    '''\n",
    "    input sentence, returns a list of words\n",
    "    '''\n",
    "    tokenlist = insent.split()\n",
    "    return tokenlist\n",
    "\n",
    "def stemming(inlist):\n",
    "    '''\n",
    "    input a list, returns a list of stemmed words\n",
    "    '''\n",
    "    outlist = []\n",
    "    stemmer = nltk.SnowballStemmer('english')\n",
    "    for word in inlist:\n",
    "        outlist.append(stemmer.stem(word))\n",
    "    return outlist\n",
    "\n",
    "def remove_stopwords(inlist):\n",
    "    '''\n",
    "    input list of words, returns list of words w/o stopwords\n",
    "    e.g.: the, a, here, we\n",
    "    '''\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    outlist = []\n",
    "    for word in inlist:\n",
    "        if word not in stop_words:\n",
    "            outlist.append(word)\n",
    "    return outlist\n",
    "\n",
    "def remove_punkt(inlist, exemption = ['@', '#']):\n",
    "    '''\n",
    "    input list of words, returns list of words w/o punctuation\n",
    "    by default, @ and # are not removed since they are important features of tweets\n",
    "    '''\n",
    "    #outlist = [word for word in inlist if not re.fullmatch('[' + string.punctuation + ']+', word)]\n",
    "    punktlist = string.punctuation\n",
    "    for e in exemption:\n",
    "        punktlist = punktlist.replace(e, '')\n",
    "    outlist = []\n",
    "    for word in inlist:\n",
    "        word = word.translate(str.maketrans('','', punktlist))\n",
    "        outlist.append(word)\n",
    "    return outlist\n",
    "\n",
    "def remove_regex(inlist, pattern = \"@[\\w]*\"):\n",
    "    '''\n",
    "    use regular expression to remove words in text.\n",
    "    removes @ someone by default.\n",
    "    inlist: a list of words\n",
    "    pattern: a list of re pattern\n",
    "    '''\n",
    "    regex = re.compile(pattern)\n",
    "    filtered_list = [x for x in inlist if not regex.match(x)]\n",
    "    return filtered_list\n",
    "\n",
    "def create_wordcount(corpus):\n",
    "    wordlist = list([a for b in corpus.tokens.tolist() for a in b])\n",
    "    from collections import Counter\n",
    "    counter = Counter(wordlist)\n",
    "    c = dict(counter)\n",
    "    # Remove spaces/invalid characters\n",
    "    c.pop('')\n",
    "    # Remove words that only appear relatively few times\n",
    "    l=sum(c.values()) # Relative frequency instead of total count\n",
    "    # Words that should stay in graph, regardless of freq. Needed to keep 'hillary'\n",
    "    important_words=['hillary','clinton','donald','trump','obama','USA','vote',\n",
    "    'elect','president','democrat','republican','democrats','republicans',\n",
    "    'crooked','emails']\n",
    "    most_common = dict(counter.most_common(30)).keys()\n",
    "    word_count={k:v*100/l for k, v in c.items() if v > l/50 or k in important_words or k in most_common}\n",
    "    #import code; code.interact(local=locals()) #DEBUG\n",
    "    return word_count\n",
    "\n",
    "def clean_data(sample_size, path = \"tweets.csv\"):\n",
    "\n",
    "    print('setting up environment...')\n",
    "    settingenv()\n",
    "    print('set up environment')\n",
    "    \n",
    "    print('loading tweets from file...')\n",
    "    tweets_df = tweets(path)\n",
    "    print('loaded tweets from file')\n",
    "    \n",
    "    print('selecting sample of tweets...')\n",
    "    corpus = sampling(count=sample_size, tweets_df=tweets_df)\n",
    "    print('selected sample of tweets')\n",
    "    \n",
    "    print('tokenizing tweets...')\n",
    "    corpus['tokens'] = corpus['text'].apply(tokenize)\n",
    "    print('tokenized tweets')\n",
    "\n",
    "    print('stemming tweets...')\n",
    "    corpus['tokens'] = corpus['tokens'].apply(stemming)\n",
    "    print('stemmed tweets')\n",
    "\n",
    "    print('removing stopwords...')\n",
    "    corpus['tokens'] = corpus['tokens'].apply(remove_stopwords)\n",
    "    print('removed stopwords')\n",
    "\n",
    "    print('removing puncutation...')\n",
    "    corpus['tokens'] = corpus['tokens'].apply(remove_punkt)\n",
    "    print('removed punctuation')\n",
    "\n",
    "    #remove all @someone\n",
    "    print('removing @someones...')\n",
    "    corpus['tokens'] = corpus['tokens'].apply(remove_regex)\n",
    "    print('removed @someones')\n",
    "\n",
    "    # remove all python links\n",
    "    print('removing links...')\n",
    "    corpus['tokens'] = corpus['tokens'].apply(remove_regex, pattern = \"https*\")\n",
    "    print('removed links')\n",
    "\n",
    "    # remove all hashtags\n",
    "    print('removing hashtags...')\n",
    "    corpus['tokens'] = corpus['tokens'].apply(remove_regex, pattern = \"#[\\w]*\")\n",
    "    print('removed hashtags')\n",
    "\n",
    "    # remove all retweets\n",
    "    print('removing retweets')\n",
    "    corpus['tokens'] = corpus['tokens'].apply(remove_regex, pattern = \"rt\")\n",
    "    print('removed retweets')\n",
    "\n",
    "    # create word count\n",
    "    print('generating word count...')\n",
    "    words=create_wordcount(corpus)\n",
    "    print('generated word count')\n",
    "\n",
    "    return words\n",
    "\n",
    "if __name__=='__main__':\n",
    "    words=clean_data(sample_size=200)\n",
    "    print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_samples(dict_one, dict_two, sample_list):\n",
    "    '''Generates a pairing between the two given dicts for each value in the given list'''\n",
    "    assert isinstance(dict_one, dict) and dict_one\n",
    "    assert isinstance(dict_two, dict) and dict_two\n",
    "    assert isinstance(sample_list, list) and sample_list\n",
    "    assert all(type(sample_list[0]) == type(key) for key in dict_one.keys())\n",
    "    assert all(type(sample_list[0]) == type(key) for key in dict_two.keys())\n",
    "    d = {s:(dict_one[s], dict_two[s]) for s in sample_list if s in dict_one and s in dict_two}\n",
    "    return d\n",
    "\n",
    "def match_error(source_dict, other_dict, sample_list):\n",
    "    ''''''\n",
    "    matched = match_samples(source_dict, other_dict, sample_list)\n",
    "    d = {k:((v[1]-v[0])/v[0]) for k, v in matched.items()}\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up environment...\n",
      "set up environment\n",
      "loading tweets from file...\n",
      "loaded tweets from file\n",
      "selecting sample of tweets...\n",
      "selected sample of tweets\n",
      "tokenizing tweets...\n",
      "tokenized tweets\n",
      "stemming tweets...\n",
      "stemmed tweets\n",
      "removing stopwords...\n",
      "removed stopwords\n",
      "removing puncutation...\n",
      "removed punctuation\n",
      "removing @someones...\n",
      "removed @someones\n",
      "removing links...\n",
      "removed links\n",
      "removing hashtags...\n",
      "removed hashtags\n",
      "removing retweets\n",
      "removed retweets\n",
      "generating word count...\n",
      "generated word count\n"
     ]
    }
   ],
   "source": [
    "trollWords = clean_data(sample_size=300, path='tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up environment...\n",
      "set up environment\n",
      "loading tweets from file...\n",
      "loaded tweets from file\n",
      "selecting sample of tweets...\n",
      "selected sample of tweets\n",
      "tokenizing tweets...\n",
      "tokenized tweets\n",
      "stemming tweets...\n",
      "stemmed tweets\n",
      "removing stopwords...\n",
      "removed stopwords\n",
      "removing puncutation...\n",
      "removed punctuation\n",
      "removing @someones...\n",
      "removed @someones\n",
      "removing links...\n",
      "removed links\n",
      "removing hashtags...\n",
      "removed hashtags\n",
      "removing retweets\n",
      "removed retweets\n",
      "generating word count...\n",
      "generated word count\n"
     ]
    }
   ],
   "source": [
    "normalWords = clean_data(sample_size=300, path='election_day_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = match_error(normalWords, trollWords, ['vote'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vote': (3.1346578366445916, 0.26881720430107525), 'trump': (1.5894039735099337, 1.657706093189964), 'hillari': (0.4856512141280353, 0.8512544802867383), 'hillary': (0.17660044150110377, 0.044802867383512544), 'clinton': (0.6181015452538632, 0.8512544802867383)}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'show_cumulative_comparison' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f563ea6c4e4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlist1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'hillari'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clinton'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hillary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlist2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'donald'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trump'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mshow_cumulative_comparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomparisons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mshow_individual_comparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomparisons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mshow_wordcloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'show_cumulative_comparison' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "comparison_words=['vote', 'trump', 'hillari', 'hillary', 'clinton', 'amp']\n",
    "comparisons=match_samples(normalWords, trollWords, comparison_words)\n",
    "print(comparisons)\n",
    "#show_comparison(comparisons)\n",
    "list1 = ['hillari', 'clinton', 'hillary']\n",
    "list2 = ['donald', 'trump']\n",
    "show_cumulative_comparison(comparisons, list1, list2)\n",
    "show_individual_comparison(comparisons, list1, list2)\n",
    "show_wordcloud(words = words)\n",
    "show_histogram(trollWords,title='Russian Twitter Bot Word Frequency')\n",
    "show_histogram(normalWords,title='User Political Tweet Word Frequency')\n",
    "show_histogram(matched,title='Word Comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~taichifox/5.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<chart_studio.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='taichifox', api_key='WdzbMDcysGxzMjJqxHUw')\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "def holoviewbar(comparisons):\n",
    "    '''generate bar graphs using holoviews'''\n",
    "    assert isinstance(comparisons, dict)\n",
    "    trace1 = go.Bar(\n",
    "        x = list(comparisons.keys())[0:5],\n",
    "        y = [i[0] for i in list(comparisons.values())][0:5],\n",
    "        name='Normal Users',\n",
    "        marker=dict(\n",
    "            color='rgb(0,147,250)',\n",
    "        )\n",
    "    )\n",
    "    trace2 = go.Bar(\n",
    "        x = list(comparisons.keys())[0:5],\n",
    "        y = [i[1] for i in list(comparisons.values())][0:5],\n",
    "        name='Russian Trolls',\n",
    "        marker=dict(\n",
    "            color='rgb(255,0,11)',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    data = [trace1, trace2]\n",
    "    layout = go.Layout(\n",
    "        barmode='group',\n",
    "        legend = dict(\n",
    "        x = 0.70,\n",
    "        y = 0.95,\n",
    "        font=dict(\n",
    "            family='Old Standard TT, serif',\n",
    "            size=23,\n",
    "            color='#000'\n",
    "            ),\n",
    "        ),\n",
    "        xaxis=go.layout.XAxis(\n",
    "           tickfont=dict(\n",
    "                family='Old Standard TT, serif',\n",
    "                size=23,\n",
    "                color='black'\n",
    "            )\n",
    "        ),\n",
    "        yaxis=go.layout.YAxis(\n",
    "            tickfont=dict(\n",
    "                family='Old Standard TT, serif',\n",
    "                size=23,\n",
    "                color='black'\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    py.iplot(fig, filename='individual_comparisons')\n",
    "\n",
    "holoviewbar(comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
